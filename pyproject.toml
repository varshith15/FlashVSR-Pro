[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "flashvsr"
version = "0.1.0"
description = "FlashVSR plugin for Daydream Scope"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "torch==2.9.1",
    "torchvision==0.24.1",
    "torchaudio==2.9.1",
    "torchmetrics",
    "torchsde",
    "accelerate>=1.1.1",
    "einops>=0.8.1",
    "huggingface-hub>=0.25.0",
    "matplotlib==3.10.3",
    "numpy==1.26.4",
    "opencv-python==4.11.0.86",
    "opencv-python-headless==4.11.0.86",
    "peft>=0.17.1",
    "pillow==11.0.0",
    "safetensors>=0.6.2",
    "sentencepiece==0.2.0",
    "transformers==4.49.0",
    "pytorch-lightning==2.5.2",
    "imageio>=2.37.0",
    "imageio-ffmpeg>=0.6.0",
    "protobuf==3.20.3",
    "ftfy>=6.3.1",
    "pandas==2.3.0",
    "tqdm",
    "datasets",
    "ffmpeg-python",
    "modelscope",
    "triton==3.5.1; sys_platform == 'linux'",
    "triton-windows==3.5.1.post24; sys_platform == 'win32'",
]

[project.optional-dependencies]
block-sparse = [
    "block-sparse-attn",
]
attention = [
    "flash-attn==2.8.3; sys_platform == 'linux' or sys_platform == 'win32'",
    "sageattention==2.2.0; sys_platform == 'linux' or sys_platform == 'win32'",
]

[project.entry-points.scope]
flashvsr = "flashvsr_scope_plugin"

[tool.uv]
override-dependencies = [
    "torch==2.9.1",
    "torchvision==0.24.1",
    "torchaudio==2.9.1",
]

# Prebuilt wheels for flash-attn and sageattention
[tool.uv.sources]
flash-attn = [
    # Prebuilt Linux wheels from https://github.com/Dao-AILab/flash-attention
    { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl", marker = "sys_platform == 'linux'" },
    # Prebuilt Windows wheels from https://github.com/kingbri1/flash-attention
    { url = "https://github.com/kingbri1/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu128torch2.9.0cxx11abiFALSE-cp312-cp312-win_amd64.whl", marker = "sys_platform == 'win32'" },
]
sageattention = [
    # Prebuilt Linux wheels from https://huggingface.co/Kijai/PrecompiledWheels
    { url = "https://huggingface.co/Kijai/PrecompiledWheels/resolve/main/sageattention-2.2.0-cp312-cp312-linux_x86_64.whl", marker = "sys_platform == 'linux'" },
    # Prebuilt Windows wheels from https://github.com/woct0rdho/SageAttention/releases
    { url = "https://github.com/woct0rdho/SageAttention/releases/download/v2.2.0-windows.post4/sageattention-2.2.0+cu128torch2.9.0andhigher.post4-cp39-abi3-win_amd64.whl", marker = "sys_platform == 'win32'" },
]
block-sparse-attn = { path = "./Block-Sparse-Attention" }

[tool.uv.extra-build-dependencies]
block-sparse-attn = ["torch==2.9.1"]

[tool.setuptools.packages.find]
where = ["."]
include = ["diffsynth*", "flashvsr_utils*", "flashvsr_scope_plugin*"]
namespaces = false

[tool.setuptools.package-dir]
flashvsr_scope_plugin = "flashvsr_scope_plugin"
